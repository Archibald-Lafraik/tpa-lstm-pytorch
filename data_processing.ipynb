{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Electricity Price Forecasting with Deep Neural Networks\n",
    "Electricity is a *basic human need* and definitely one of the most important factors of societal progress. In recent decades however, electricity has entered the market as a tradeable commodity and the power industry of many countries has been **deregulated**. In Spain, the Electric Power Act 54/1997 exposed all of the stakeholders to **high amounts of uncertainty** as the price of electricity is determined by countless factors and also, due to the fact that electricity cannot be stored in large quantities efficiently [[1]](#ref1). With the emergence of this new market, the need for reliable forecasting methods at all scales (hourly, daily, long-term, etc.) has also emerged and has become a large area of research.\n",
    "\n",
    "The goal of this kernel is to compare different **deep neural network architectures** (+XGBoost) on the task of **predicting the next hour's electricity price** by using the past values of the electricity price as well as those of another features related to energy generation and weather conditions. Furthermore, the kernel contains a meticulous *exploration and cleaning* of the data, *time series analysis* of the electricity price and careful *feature engineering*. With further research and development (e.g. as a forecasting model which is updated in real-time) a similar approach could possibly prove useful to all the stakeholders (electric power companies, investors, etc.) involved in energy markets. For the time being, I believe that this kernel can serve as a **future reference** for aspiring data scientists, as it contains an *end-to-end time series forecasting project* and demonstrates the -minimum- level of immersion that a dataset needs in order to turn into something which is actually applicable in the real world.\n",
    "\n",
    "In the **original project**, which was conducted for a postgraduate course, I compared the performance (using the Root Mean Squared Error as the performance metric) of 5 different architectures (LSTM, stacked LSTM, CNN, CNN-LSTM and Time Distributed MLP) for both univariate and multivariate forecasting (i.e. using only the previous time-steps of the electricity price vs. also using other features) using a different number of previous time-steps as the features for the models (3, 10 and 25 previous time-steps for all the used features). In **this particular kernel**, you will find an application of all the aforementioned deep learning architecures, as well as two more approaches: the *Encoder-Decoder* architecure and the *XGBoost regressor*. Furthermore, in all these applications, I use the 25 previous time-steps of all the features (multivariate forecasting) that have been extracted or generated, after applying *PCA (Principal Component Analysis)*. The **Adam optimizer** is used in all the deep learning architectures and, in order to choose its learning rate, I originally conducted preliminary tests using the the *learning rate scheduler* callback starting from a learning rate equal to 0.0001 and gradually increasing it by a factor of 10 every 10 epochs; a step which is omitted in this kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [<font size=\"5\">1. Exploration and Cleaning</font>](#ref2)\n",
    "\n",
    "    - [<font size=\"4\">1.1. Energy dataset</font> ](#ref3)\n",
    "\n",
    "    - [<font size=\"4\">1.2. Weather features dataset</font> ](#ref4)\n",
    "\n",
    "    - [<font size=\"4\">1.3. Merging the two datasets</font>](#ref11)\n",
    "    \n",
    "    \n",
    "- [<font size=\"5\">2. Visualizations and Time Series Analysis</font>](#ref30)\n",
    "\n",
    "    - [<font size=\"4\">2.1. Useful visualizations and insights</font>](#ref12)\n",
    "    \n",
    "    - [<font size=\"4\">2.2. Decomposition and stationarity tests</font>](#ref13)\n",
    "    \n",
    "    - [<font size=\"4\">2.3. Autocorrelation, partial autocorrelation and cross-correlation</font>](#ref14)\n",
    "    \n",
    "    \n",
    "- [<font size=\"5\">3. Feature Engineering</font>](#ref15)\n",
    "\n",
    "    - [<font size=\"4\">3.1. Feature generation</font>](#ref16)\n",
    "\n",
    "    - [<font size=\"4\">3.2. Feature selection</font>](#ref17)\n",
    "    \n",
    "    \n",
    "- [<font size=\"5\">4. Electricity Price Forecasting</font>](#ref18)\n",
    "\n",
    "    - [<font size=\"4\">4.1. Naive forecast</font>](#ref19)\n",
    "\n",
    "    - [<font size=\"4\">4.2. XGBoost</font>](#ref20)\n",
    "\n",
    "    - [<font size=\"4\">4.3. LSTM</font>](#ref21)\n",
    "    \n",
    "    - [<font size=\"4\">4.4. Stacked LSTMs</font>](#ref22)\n",
    "\n",
    "    - [<font size=\"4\">4.5. CNN</font>](#ref23)\n",
    "\n",
    "    - [<font size=\"4\">4.6. CNN-LSTM</font>](#ref24)\n",
    "    \n",
    "    - [<font size=\"4\">4.7. Time Disitributed MLP</font>](#ref25)\n",
    "\n",
    "    - [<font size=\"4\">4.8. Encoder-Decoder</font>](#ref26)\n",
    "    \n",
    "\n",
    "- [<font size=\"5\">5. Results</font>](#ref27)\n",
    "\n",
    "\n",
    "- [<font size=\"5\">6. References</font>](#ref10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msm\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#import xgboost as xgb\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "#import xgboost as xgb\n",
    "import os\n",
    "import warnings\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, TimeDistributed, Flatten, Dropout, RepeatVector\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller, kpss, ccf\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "\n",
    "from dataset import ElectricityDataModule\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=(FutureWarning, UserWarning))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='ref2'>1. Exploration and Cleaning</a>\n",
    "In this dataset, we have two .csv files which contain hourly information about the electricity generation and weather in Spain for the period 2015-2019 (4 years). In particular:\n",
    "\n",
    "> **`'weather_features.csv':`** Contains hourly information about the weather conditions (e.g. temperature, wind speed, humidity, rainfall, qualitative desctiption) of 5 major cities in Spain (Madrid, Barcelona, Valencia, Seville and Bilbao).\n",
    "\n",
    "> **`'energy_dataset.csv':`** Contains hourly information about the generation of energy in Spain. In particular, there is info (in MW) about the amount of electricty generated by the various energy sources (fossil gas, fossil hard coal and wind energy dominate the energy grid), as well as about the total load (energy demand) of the national grid and the price of energy (&euro;/MWh). \n",
    "_Note: Since the generation of each energy type is in MW and the time-series contains hourly info, the value of each cell represents MWh (Megawatt hours)._\n",
    "\n",
    "The information that we have about the weather of 5 major cities in Spain (highlighted by a red star on the map below) is probably more than enough for our analysis, since their geographic distribution covers most of the part of Spain's territory in a uniform manner. Moreover, it is useful to note that these 5 cities alone comprise approximately 1/3rd of the total population of Spain.\n",
    "\n",
    "![Highlighted cities for which we have weather information](https://i.imgur.com/Hfb9vmq.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the datasets\n",
    "\n",
    "df_weather = pd.read_csv(\n",
    "    'data/weather_features.csv', \n",
    "    parse_dates=['dt_iso']\n",
    ")\n",
    "\n",
    "df_energy = pd.read_csv(\n",
    "    'data/energy_dataset.csv', \n",
    "    parse_dates=['time']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref3'>1.1. Energy dataset</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will drop all the columns that are constituted by zeroes and NaNs, as they are unusable. We will also remove the columns which will not be used at all in our analysis and which contain day-ahead forecasts for the total load, the solar energy and the wind energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unusable columns\n",
    "\n",
    "df_energy = df_energy.drop(['generation fossil coal-derived gas','generation fossil oil shale', \n",
    "                            'generation fossil peat', 'generation geothermal', \n",
    "                            'generation hydro pumped storage aggregated', 'generation marine', \n",
    "                            'generation wind offshore', 'forecast wind offshore eday ahead',\n",
    "                            'total load forecast', 'forecast solar day ahead',\n",
    "                            'forecast wind onshore day ahead'], \n",
    "                            axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_energy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'time' column, which we also want to function as the index of the observations in a time-series, has not been parsed correctly and is recognized as an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time to datetime object and set it as index\n",
    "\n",
    "df_energy['time'] = pd.to_datetime(df_energy['time'], utc=True, infer_datetime_format=True)\n",
    "df_energy = df_energy.set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find NaNs and duplicates in df_energy\n",
    "\n",
    "print('There are {} missing values or NaNs in df_energy.'\n",
    "      .format(df_energy.isnull().values.sum()))\n",
    "\n",
    "temp_energy = df_energy.duplicated(keep='first').sum()\n",
    "\n",
    "print('There are {} duplicate rows in df_energy based on all columns.'\n",
    "      .format(temp_energy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `df_energy` has no duplicate values. Nevertheless, it has some NaNs and thus, we have to investigate further. Since this is a time-series forecasting task, we cannot simply drop the rows with the missing values and it would be a better idea to fill the missing values using interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of NaNs in each column\n",
    "\n",
    "df_energy.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most null values can be found in the 'total load actual' column. Therefore, it is a good idea to visualize it and see what we can do. The good news is that there are no NaNs in the 'price actual' column, which we will use as the target variable in order to train our model. The similar numbers in null values in the columns which have to do with the type of energy generation *probably indicate that they will also appear in the same rows*. Let us first define a plot function which we will then use so as to visualize the 'total load actual' column, as well as other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot different types of time-series\n",
    "\n",
    "def plot_series(df=None, column=None, series=pd.Series([]), \n",
    "                label=None, ylabel=None, title=None, start=0, end=None):\n",
    "    \"\"\"\n",
    "    Plots a certain time-series which has either been loaded in a dataframe\n",
    "    and which constitutes one of its columns or it a custom pandas series \n",
    "    created by the user. The user can define either the 'df' and the 'column' \n",
    "    or the 'series' and additionally, can also define the 'label', the \n",
    "    'ylabel', the 'title', the 'start' and the 'end' of the plot.\n",
    "    \"\"\"\n",
    "    sns.set()\n",
    "    fig, ax = plt.subplots(figsize=(30, 12))\n",
    "    ax.set_xlabel('Time', fontsize=16)\n",
    "    if column:\n",
    "        ax.plot(df[column][start:end], label=label)\n",
    "        ax.set_ylabel(ylabel, fontsize=16)\n",
    "    if series.any():\n",
    "        ax.plot(series, label=label)\n",
    "        ax.set_ylabel(ylabel, fontsize=16)\n",
    "    if label:\n",
    "        ax.legend(fontsize=16)\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=24)\n",
    "    ax.grid(True)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoom into the plot of the hourly (actual) total load\n",
    "\n",
    "ax = plot_series(df=df_energy, column='total load actual', ylabel='Total Load (MWh)',\n",
    "                 title='Actual Total Load (First 2 weeks - Original)', end=24*7*2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After zooming into the first 2 weeks of the 'total load actual' column, we can already see that there are null values for a few hours. However, the number of the missing values and the behavior of the series indicate that an interpolation would fill the NaNs quite well. Let us further investigate if the null values coincide across the different columns. Let us display the last five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the rows with null values\n",
    "\n",
    "df_energy[df_energy.isnull().any(axis=1)].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we manually searched through all of them, we would confirm that the null values in the columns which have to do with the type of energy generation mostly coincide. The null values in 'actual total load' also coincide with the aforementioned columns, but also appear in other rows as well. In order to handle the null values in `df_energy`, we will use a linear interpolation with a forward direction. Perhaps other kinds of interpolation would be better; nevertheless, we prefer to use the simplest model possible. Only a small part of our input data will be noisy and it will not affect performance noticeably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values using interpolation\n",
    "\n",
    "df_energy.interpolate(method='linear', limit_direction='forward', inplace=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of non-zero values in each column\n",
    "\n",
    "print('Non-zero values in each column:\\n', df_energy.astype(bool).sum(axis=0), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It look like `df_energy` has been cleaned successfully and is ready for further use as input into our model. The 1-4 zeroes in the columns which have to do with energy generation by type should not concern us very much. The 'generation hydro pumped storage consumption' may look suspicious, but we should have in mind that this type of energy is only used for load balancing, being consumed when in peak energy demands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref4'>1.2. Weather features dataset</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that all columns of `df_weather` have the same number of rows; we still have to check what is the case for each city individually, though. We should note that the temperatures are in Kelvin. The most important thing to notice is that there are some **problems** and **outliers**. In particular:\n",
    "- There is at least one outlier in the 'pressure' column as the maximum value is 1,008,371 hPa or approximately 100 MPa, which is roughly the pressure at the bottom of Mariana Trench about 11 km below ocean surface [[2]](#ref5). This cannot be the case here.\n",
    "- There is at least one outlier in the 'wind_speed' column as the maximum value is 133 m/s. This measurement is close to the fastest wind speed ever recorded on Earth, caused by the 1999 Bridge Creek–Moore tornado [[3]](#ref6), a F5 (the largest intensity of the Fujita scale) tornado [[4]](#ref7). A tornado of such intensity has not been recorded in Spain [[5]](#ref9) and hopefully will not happen in the future as well. \n",
    "- The 'rain_3h' column is supposed to provide information about the precipitation (i.e. rain) of the last 3 hours in mm. Since the 'rain_1h' column is supposed to provide the same information but about just the last hour, it would be logical to assume that its mean would be less than that of 'rain_3h'. However, this is not the case in the statistical description above. So, it would be a good idea to further examine those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the type of each variable in df_weather\n",
    "\n",
    "df_weather.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to change the type of some of the columns, so that all of them are float64. We also have to parse 'dt_iso' correctly and actually rename it as 'time' so that it matches with the index of `df_energy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_convert_dtypes(df, convert_from, convert_to):\n",
    "    cols = df.select_dtypes(include=[convert_from]).columns\n",
    "    for col in cols:\n",
    "        df[col] = df[col].values.astype(convert_to)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns with int64 type values to float64 type\n",
    "\n",
    "df_weather = df_convert_dtypes(df_weather, np.int64, np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dt_iso to datetime type, rename it and set it as index\n",
    "\n",
    "df_weather['time'] = pd.to_datetime(df_weather['dt_iso'], utc=True, infer_datetime_format=True)\n",
    "df_weather = df_weather.drop(['dt_iso'], axis=1)\n",
    "df_weather = df_weather.set_index('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to **split** the `df_weather` dataset into 5 datasets, **one for each different city** (Madrid, Barcelona, Bilbao, Seville and Valencia). But first, let's see the average values for each column, grouped by each city (note that the 'weather_id' average has no meaning whatsoever)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display average weather features grouped by each city\n",
    "\n",
    "mean_weather_by_city = df_weather.groupby('city_name').mean()\n",
    "mean_weather_by_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find NaNs and duplicates in df_weather\n",
    "\n",
    "print('There are {} missing values or NaNs in df_weather.'\n",
    "      .format(df_weather.isnull().values.sum()))\n",
    "\n",
    "temp_weather = df_weather.duplicated(keep='first').sum()\n",
    "\n",
    "print('There are {} duplicate rows in df_weather based on all columns.'\n",
    "      .format(temp_weather))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `df_weather` has a lot of *duplicate values*. However, **the method above may also show us rows which have the exame same values**. This is not what we are looking for. What we want to ensure, is that **there are no duplicate index rows**, i.e. that we do not have multiple rows **for the exact same hour**. Of course, we also have to make sure that these duplicates concern **each individual city**. Since, `df_weather` contains information about 5 different cities, it is very useful to display the number of observations for each one and compare it with the size of `df_energy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of rows in each dataframe\n",
    "\n",
    "print('There are {} observations in df_energy.'.format(df_energy.shape[0]))\n",
    "\n",
    "cities = df_weather['city_name'].unique()\n",
    "grouped_weather = df_weather.groupby('city_name')\n",
    "\n",
    "for city in cities:\n",
    "    print('There are {} observations in df_weather'\n",
    "          .format(grouped_weather.get_group('{}'.format(city)).shape[0]), \n",
    "          'about city: {}.'.format(city))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the two dataframes (`df_energy` and `df_weather`) cannot be merged yet. There are many duplicates for every city in `df_weather` and we should drop them and see if their number of rows match. We do this by resetting the index, keeping only the **first rows** which have the same 'time' and 'city_name' values and then setting again 'time' as the index. For further research on the dataset, let us also create a second dataframe, `df_weather_2` in which we do the same procedure, but keep only the **last rows** which have the same 'time' and 'city_name'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df_weather_2 and drop duplicate rows in df_weather\n",
    "\n",
    "df_weather_2 = df_weather.reset_index().drop_duplicates(subset=['time', 'city_name'], \n",
    "                                                        keep='last').set_index('time')\n",
    "\n",
    "df_weather = df_weather.reset_index().drop_duplicates(subset=['time', 'city_name'],\n",
    "                                                      keep='first').set_index('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of rows in each dataframe again\n",
    "\n",
    "print('There are {} observations in df_energy.'.format(df_energy.shape[0]))\n",
    "\n",
    "grouped_weather = df_weather.groupby('city_name')\n",
    "\n",
    "for city in cities:\n",
    "    print('There are {} observations in df_weather'\n",
    "          .format(grouped_weather.get_group('{}'.format(city)).shape[0]), \n",
    "          'about city: {}.'.format(city))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column 'weather_icon' is irrelevant for our analysis, so we will drop it. Furthermore, the columns 'weather_main' and 'weather_description' contain approximately the same information as the column 'weather_id'; the information concerns a qualitative description of the weather at the given hour. So, we will work with only one of them. However, in order to make a choice, we have to check the **unique values** as well as the **consistency** of each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all the unique values in the column 'weather_description'\n",
    "\n",
    "weather_description_unique = df_weather['weather_description'].unique()\n",
    "weather_description_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all the unique values in the column 'weather_main'\n",
    "\n",
    "weather_main_unique = df_weather['weather_main'].unique()\n",
    "weather_main_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all the unique values in the column 'weather_id'\n",
    "\n",
    "weather_id_unique = df_weather['weather_id'].unique()\n",
    "weather_id_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in terms of qualitative description, the 'weather_main' column seems to contain the less detailed -or \"poorest\"- information, while 'weather_id' and 'weather_description' have more complex information and approximately the same number of unique values.\n",
    "\n",
    "Nevertheless, it is also useful to check the consistency of the information in each column. Since our dataset contained duplicate rows and we utilized two different methods for cleaning it, a very good way to check the consistency of the data in these three columns would be to compare the two cleaned dataframes, `df_weather` and `df_weather_2`. In order to do this, we employ the R² (\"R-squared\" or \"coefficient of determination\") metric, after encoding 'weather_description' and 'weather_main' from strings to numerical labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function which will calculate R-squared score for the same column in two dataframes\n",
    "\n",
    "def encode_and_display_r2_score(df_1, df_2, column, categorical=False):\n",
    "    dfs = [df_1, df_2]\n",
    "    if categorical:\n",
    "        for df in dfs:\n",
    "            le = LabelEncoder()\n",
    "            df[column] = le.fit_transform(df[column])\n",
    "    r2 = r2_score(df_1[column], df_2[column])\n",
    "    print(\"R-Squared score of {} is {}\".format(column, r2.round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the R-squared scores for the columns with qualitative weather descriptions in df_weather and df_weather_2\n",
    "\n",
    "encode_and_display_r2_score(df_weather, df_weather_2, 'weather_description', categorical=True)\n",
    "encode_and_display_r2_score(df_weather, df_weather_2, 'weather_main', categorical=True)\n",
    "encode_and_display_r2_score(df_weather, df_weather_2, 'weather_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we have found incosistencies in our dataset. In particular, all three of the columns which contain qualitative info for the condition of the weather seem to contain a large part of the duplicates that we found earlier in `df_weather`. From the above R-squared scores, it seems that keeping and one-hot encoding 'weather_description' or 'weather_main' is the best idea to get the most out of information in the dataset. Note that **the R² values of these two are not comparable** because we have first encoded them; in order to actually calculate the similarity between two strings, we need to employ a different metric such as the *Levenshtein distance*, i.e. the number of edits required to transform string A to string B. In this case, even this metric would not be enough in order to make 'weather_main' and 'weather_description' comparable as the former contains only single words while the latter large phrases which describe the weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with qualitative weather information\n",
    "df_weather = df_weather.drop(['weather_main', 'weather_id', \n",
    "                              'weather_description', 'weather_icon'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to emphasize that the method we employed above in order to check the consistency of our dataset **is not 100% accurate**. This is because it compares two dataframes from which we have cleaned the duplicates in **only two ways exclusively**; keeping the first rows and keeping the last rows of the duplicates. Nevertheless, it gives good results and can be used to check the values of the other columns as well. We should not that since we removed the duplicates based on all the columns except 'time' and 'city_name', there is no point in checking the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the R-squared for all the columns in df_weather and df_weather_2\n",
    "\n",
    "df_weather_cols = df_weather.columns.drop('city_name')\n",
    "\n",
    "for col in df_weather_cols:\n",
    "    encode_and_display_r2_score(df_weather, df_weather_2, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above confirms our intuition that 'weather_description', 'weather_main' and 'weather_id' are the sole culprits for the duplicates in `df_weather` and although we are not absolutely certain for this (for the reasons we described above), it is more than enough to reaffirm the validity and consistency of the rest of the columns in `df_weather` which contain numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of duplicates in df_weather\n",
    "\n",
    "temp_weather = df_weather.reset_index().duplicated(subset=['time', 'city_name'], \n",
    "                                                   keep='first').sum()\n",
    "print('There are {} duplicate rows in df_weather ' \\\n",
    "      'based on all columns except \"time\" and \"city_name\".'.format(temp_weather))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to the next step, i.e. merging `df_energy` and `df_weather`, we also want to treat the outliers which we found earlier in 'pressure' and 'wind_speed'. We will visualize the outliers in these columns using boxplot, change their values to NaNs and then use a linear interpolation in order to replace their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in 'pressure' column\n",
    "\n",
    "sns.boxplot(x=df_weather['pressure'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even a pressure of approximately 100,000 HPa or 10 MPa, which is clearly visible in the above figure, corresponds to a quantity greater than the atmospheric pressure of Venus [[2]](#ref5). In order to be sure, we will set as **NaN** every value in the 'pressure' column which is **higher than 1051 hPa**, which is just above the highest air pressure ever recorded in the Iberian peninsula [[6]](#ref9). While outliers on the low side are not visible in the boxplot above, it is a good idea to also replace the values which are **lower than 931 hPa**, i.e. the lowest air pressure ever recorded in the Iberian peninsula [[6]](#ref9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outliers in 'pressure' with NaNs\n",
    "\n",
    "df_weather.loc[df_weather.pressure > 1051, 'pressure'] = np.nan\n",
    "df_weather.loc[df_weather.pressure < 931, 'pressure'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in 'pressure' column again\n",
    "\n",
    "sns.boxplot(x=df_weather['pressure'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things look a lot better regarding atmospheric pressure ('pressure') as the column has been cleaned of extreme-case outliers. What about 'wind_speed' however?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in 'wind_speed' column\n",
    "\n",
    "sns.boxplot(x=df_weather['wind_speed'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will follow a conservative approach as above. We will set as **NaNs** the values in 'wind_speed' which are **higher than 50 m/s**, which is the highest bound (112 mph) of the wind speed estimate of a F1 (Fujita scale) tornado [[4]](#ref3), having in mind that the last F1 tornado in Spain was Gandia tornado, which was recorded on 28th September 2012 [[5]](#ref4), i.e. roughly 2 years before the starting point of our available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace outliers in 'wind_speed' with NaNs\n",
    "\n",
    "df_weather.loc[df_weather.wind_speed > 50, 'wind_speed'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in 'wind_speed' column again\n",
    "\n",
    "sns.boxplot(x=df_weather['wind_speed'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values using interpolation\n",
    "\n",
    "df_weather.interpolate(method='linear', limit_direction='forward', inplace=True, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks nice and clean and `df_weather` is also ready for use as input to our model. We also have to plot the 'rain_1h' and 'rain_3h' columns, as we have indicated that at least one of them (probably 'rain_3h') is quite problematic. Nevertheless, we will do so after we merge the two datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref11'>1.3. Merging the two datasets</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the df_weather into 5 dataframes (one for each city)\n",
    "\n",
    "df_1, df_2, df_3, df_4, df_5 = [x for _, x in df_weather.groupby('city_name')]\n",
    "dfs = [df_1, df_2, df_3, df_4, df_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataframes into the final dataframe\n",
    "\n",
    "df_final = df_energy\n",
    "\n",
    "for df in dfs:\n",
    "    city = df['city_name'].unique()\n",
    "    city_str = str(city).replace(\"'\", \"\").replace('[', '').replace(']', '').replace(' ', '')\n",
    "    df = df.add_suffix('_{}'.format(city_str))\n",
    "    df_final = df_final.merge(df, on=['time'], how='outer')\n",
    "    df_final = df_final.drop('city_name_{}'.format(city_str), axis=1)\n",
    "    \n",
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of NaNs and duplicates in the final dataframe\n",
    "\n",
    "print('There are {} missing values or NaNs in df_final.'\n",
    "      .format(df_final.isnull().values.sum()))\n",
    "\n",
    "temp_final = df_final.duplicated(keep='first').sum()\n",
    "\n",
    "print('\\nThere are {} duplicate rows in df_energy based on all columns.'\n",
    "      .format(temp_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='ref30'>2. Visualizations and Time Series Analysis</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref12'>2.1. Useful visualizations and insights</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before anything else, we will check the 'rain_1h' and 'rain_3h' columns for a specific city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 'rain_1h' for Bilbao\n",
    "\n",
    "ax = plot_series(df_final, 'rain_1h_Bilbao', \n",
    "                 label='Hourly', ylabel='Actual Price (€/MWh)',\n",
    "                 title='Rain in the last hour in Bilbao')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 'rain_3h' for Bilbao\n",
    "\n",
    "ax = plot_series(df_final, 'rain_3h_Bilbao', \n",
    "                 label='Hourly', ylabel='Rain in last 3 hours (mm)',\n",
    "                 title='Rain in the last 3 hours in Bilbao')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two figures aboce, we can conclude that **'rain_3h' is an unreliable feature which will drop from the dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['Barcelona', 'Bilbao', 'Madrid', 'Seville', 'Valencia']\n",
    "\n",
    "for city in cities:\n",
    "    df_final = df_final.drop(['rain_3h_{}'.format(city)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the hourly actual electricity price, along with the weekly rolling mean\n",
    "\n",
    "rolling = df_final['price actual'].rolling(24*7, center=True).mean()\n",
    "ax = plot_series(df_final, 'price actual', label='Hourly', ylabel='Actual Price (€/MWh)',\n",
    "                 title='Actual Hourly Electricity Price and Weekly rolling mean')\n",
    "ax.plot(rolling, linestyle='-', linewidth=2, label='Weekly rolling mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the electricity price (monthly frequence) along with its 1-year lagged series\n",
    "\n",
    "monthly_price = df_final['price actual'].asfreq('M')\n",
    "ax = plot_series(series=monthly_price, ylabel='Actual Price (€/MWh)',\n",
    "                 title='Actual electricity price (Monthly frequency) and 1-year lagged price')\n",
    "shifted = df_final['price actual'].asfreq('M').shift(12)\n",
    "ax.plot(shifted, label='Hourly')\n",
    "ax.legend(['Actual Price', '1-year Lagged Actual Price'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the (resampled) monthly frequency of the actual electricity price, along with its 1-year lagged monthly frequency. From this, we can see that there are indeed **seasonal patterns at the monthly scale**, as certain \"spikes\" in the time-series take place in exactly the same months. This means that it would be a good idea to create a new feature for the months. If the figure displayed the 1-year lagged actual energy price at hourly or weekly time scales, we would -of course- see no such seasonal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual electricity price at a daily/weekly scale\n",
    "\n",
    "ax = plot_series(df_final, 'price actual', label='Hourly', ylabel='Actual Price (€/MWh)',\n",
    "                 start=1 + 24 * 500, end=1 + 24 * 515,\n",
    "                 title='Actual Hourly Electricity Price (Zoomed - 2 Weeks)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we have plotted that actual hourly electricity price from 15/06/2016 (Sunday) at 00:00 up to 29/06/2016 (Sunday) at 23:00, i.e. two weeks of data. We can observe that there are many patterns and periodicities, such as:\n",
    "- A periodicity from week to week, as the electricity price tends to be higher during business days and lower during weekends and especially during sundays.\n",
    "- An intradyay periodicity, as the price is higher during the day and lower during the night.\n",
    "- A periodicity within the business hours, as in some cases the electricity price drops for a few hours, which is probably due to \"siesta\", the traditional lunch break between 01:30PM and 04:30PM, as Spain does not strictly follow the 9AM-5PM business day.\n",
    "\n",
    "Later on, we will make sure that we will generate features which contain these kinds of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the percentage of the hourly change in the actual electricity price\n",
    "\n",
    "change = df_energy['price actual'].div(df_energy['price actual'].shift(1)).mul(100)\n",
    "ax = plot_series(series=change, ylabel='Hourly Change (%)', \n",
    "                 title='Percentage of the hourly change in the actual electricty price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure above, we can see that the change in the actual price from hour to hour is in most of the cases between -25% (actual price \\* 0.75) and +25% (actual price \\* 1.25). However, there are also very few outliers which show that the price gets halved (-50%) or doubled (+100%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histogram of the actual electricity price\n",
    "\n",
    "ax = df_energy['price actual'].plot.hist(bins=18, alpha=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we can see that the actual energy price roughly follows a normal distribution and thus, could be standardized. However, we also have to make sure that the time series does not require any other kinds of transformations. More specifically, we will check whether the time series of energy price is stationary, after visualizing its decomposed component time-series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref13'>2.2. Decomposition and stationarity tests</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the electricity price time series\n",
    "\n",
    "res = sm.tsa.seasonal_decompose(df_energy['price actual'], model='additive')\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(20, 12))\n",
    "res.observed.plot(ax=ax1, title='Observed')\n",
    "res.trend.plot(ax=ax2, title='Trend')\n",
    "res.resid.plot(ax=ax3, title='Residual')\n",
    "res.seasonal.plot(ax=ax4, title='Seasonal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the log electricity price time-series\n",
    "\n",
    "res = sm.tsa.seasonal_decompose(np.log(df_energy['price actual']), model='additive')\n",
    "\n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(20, 12))\n",
    "res.observed.plot(ax=ax1, title='Observed')\n",
    "res.trend.plot(ax=ax2, title='Trend')\n",
    "res.resid.plot(ax=ax3, title='Residual')\n",
    "res.seasonal.plot(ax=ax4, title='Seasonal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Augmented Dickey-Fuller (ADF) test**, a type of unit root test, determines how strongly a time series is defined by a trend. Its hypotheses are the following:\n",
    "- **Null Hypothesis** $H_{0}$: There is a unit root in the time series, i.e. the series is autocorrelated with (r=1), a time dependent structure and thus, is not stationary.\n",
    "- **Alternate Hypothesis** $H_{1}$: The time series has no unit root and is either stationary or can be made stationary using differencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_final['price actual']\n",
    "adf_test = adfuller(y, regression='c')\n",
    "print('ADF Statistic: {:.6f}\\np-value: {:.6f}\\n#Lags used: {}'\n",
    "      .format(adf_test[0], adf_test[1], adf_test[2]))\n",
    "for key, value in adf_test[4].items():\n",
    "    print('Critical Value ({}): {:.6f}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ADF statistic (-9.147) is less than the critical value at 1% (-3.431) and thus, we can say that **we reject the null hypothesis $H_{0}$ with a significance level 1%**, meaning that there is not a root-unit in the time series and thus, that it is either stationary or could be made stationary with 1st order differencing (difference-stationary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test**, follows the opposite logic from Augmented Dickey-Fuller test and checks for  stationarity. Its hypotheses are the following:\n",
    "- **Null Hypothesis** $H_{0}$: The time series is level, i.e. it is stationary around a constant.\n",
    "- **Alternate Hypothesis** $H_{1}$: There is a unit root in the time series and thus it not stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpss_test = kpss(y, regression='c', nlags='legacy')\n",
    "print('KPSS Statistic: {:.6f}\\np-value: {:.6f}\\n#Lags used: {}'\n",
    "      .format(kpss_test[0], kpss_test[1], kpss_test[2]))\n",
    "for key, value in kpss_test[3].items():\n",
    "    print('Critical Value ({}): {:.6f}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KPSS statistic (7.957) is higher than the critical value at 1% (0.739) and thus, we can say that **we cannot reject the null hypothesis $H_{0}$ with a significance level 1%**, meaning that the time series is stationary or stationary around a constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both tests concluded that the electricity price time series is stationary, while it is also true that deep neural networks can handle such properties in a more forgiving way compared to ARIMA models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref14'>2.3. Autocorrelation, partial autocorrelation and cross-correlation</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot autocorrelation and partial autocorrelation plots\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, figsize=(10, 6))\n",
    "plot_acf(df_final['price actual'], lags=50, ax=ax1)\n",
    "plot_pacf(df_final['price actual'], lags=50, ax=ax2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial autocorrelation plot of the eletricity price time series shows that the direct relationship between an observation at a given hour (t) is strongest with the observations at t-1, t-2, t-24 and t-25 time-steps and diminishes afterwards. Thus, we are going to use the 25 previous values of each time series which will constitute a feature for our models. \n",
    "\n",
    "Nevertheless, it would quite definitely be more beneficial if we only chose to use specific past values (observations at certain time-lags) of a given feature, based on the cross-correlation between the electricity price and each one of the features in the dataset. For example, below we can see the cross-correlation between the electricity price and the total load. We see that there are many time-lags with a correlation which is close to zero and could be ommited. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_corr = ccf(df_final['total load actual'], df_final['price actual'])\n",
    "plt.plot(cross_corr[0:50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will just work with the correlations between the electricity price and the other features at each given hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the correlations between the electricity price and the rest of the features\n",
    "\n",
    "correlations = df_final.corr(method='pearson')\n",
    "print(correlations['price actual'].sort_values(ascending=False).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can already see some very interesting correlations among the energy price that we want to predict and the rest of the features. For example, the total energy load and the amount of energy generated from sources related to fossil fuels, is positively correlated with the electricity price. In contrast, the wind speed in almost every city and the amount of storage energy cosumed through hydroelectric pumping is negatively correlated with the energy price. We will go ahead and drop the 'snow_3h_Barcelona' and 'snow_3h_Seville' which give NaNs in their correlations with the electricity actual price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop(['snow_3h_Barcelona', 'snow_3h_Seville'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Pearson correlation matrix\n",
    "\n",
    "correlations = df_final.corr(method='pearson')\n",
    "fig = plt.figure(figsize=(24, 24))\n",
    "sns.heatmap(correlations, annot=True, fmt='.2f')\n",
    "plt.title('Pearson Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we cannot make a lot out of the above correlation matrix, we can observe that there quite a lot of features that are highly correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highly_correlated = abs(correlations[correlations > 0.75])\n",
    "print(highly_correlated[highly_correlated < 1.0].stack().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can observe, is that apart from the amount of energy generated by fossil brown coal/lignit and hard coal, the most correlated features have to do with the temperatures among the different cities, as well the \"secondary\" information we have about the temperature of each individual city (i.e. the minimunm and maximum temperature)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='ref15'>3. Feature Engineering</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref16'>3.1. Feature generation</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first features that we will generate for a given hour will just be that particular hour, the day and the month in which it falls into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'hour', 'weekday' and 'month' features\n",
    "\n",
    "for i in range(len(df_final)):\n",
    "    position = df_final.index[i]\n",
    "    hour = position.hour\n",
    "    weekday = position.weekday()\n",
    "    month = position.month\n",
    "    df_final.loc[position, 'hour'] = hour\n",
    "    df_final.loc[position, 'weekday'] = weekday\n",
    "    df_final.loc[position, 'month'] = month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very useful feature that we will generate, has to do with the business hours, i.e. whether businesses are open or not in a given hour. However, there is a certain peculiarity in Spain as the 9AM-5PM working day is not generally followed by all types of businesses, due to a lunch break (known as \"siesta\") in between. The most usual business hours are from Monday-Saturday, from 9:30AM-1:30PM and then again from 4:30PM-8PM.\n",
    "\n",
    "Therefore, the value of the 'business hour' will be equal to '2' if the given hour is within the business hours, equal to '1' if the given hour is within the \"siesta\" in between and equal to '0' for all other given hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'business hour' feature\n",
    "\n",
    "for i in range(len(df_final)):\n",
    "    position = df_final.index[i]\n",
    "    hour = position.hour\n",
    "    if ((hour > 8 and hour < 14) or (hour > 16 and hour < 21)):\n",
    "        df_final.loc[position, 'business hour'] = 2\n",
    "    elif (hour >= 14 and hour <= 16):\n",
    "        df_final.loc[position, 'business hour'] = 1\n",
    "    else:\n",
    "        df_final.loc[position, 'business hour'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we have generated the 'business hour' feature in such a way which also includes weekends, on which fewer -or different types of- businesses are open. Thus, it is useful to also generate another feature, 'weekend', which will distuingish weekdays and weekends, as well as make a distinction between Saturdays and Sundays.\n",
    "\n",
    "In particular, for a particular given hour, the value of 'weekend' will be equal to '0', if the hours is in a weekday, equal to '1' if the hour is in a Saturday and equal to '2' if the hour is in a Sunday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'weekend' feature\n",
    "\n",
    "for i in range(len(df_final)):\n",
    "    position = df_final.index[i]\n",
    "    weekday = position.weekday()\n",
    "    if (weekday == 6):\n",
    "        df_final.loc[position, 'weekday'] = 2\n",
    "    elif (weekday == 5):\n",
    "        df_final.loc[position, 'weekday'] = 1\n",
    "    else:\n",
    "        df_final.loc[position, 'weekday'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce the dimensionality and potentially acquire a new kind of information, for every given hour, we will subtract the minimum temperature ('temp_min') from the maximum temperature ('temp_max') for each city and we will name that feature 'temp_range_{name of city}'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'temp_range' for each city\n",
    "\n",
    "cities = ['Barcelona', 'Bilbao', 'Madrid', 'Seville', 'Valencia']\n",
    "\n",
    "for i in range(len(df_final)):\n",
    "    position = df_final.index[i]\n",
    "    for city in cities:\n",
    "        temp_max = df_final.loc[position, 'temp_max_{}'.format(city)]\n",
    "        temp_min = df_final.loc[position, 'temp_min_{}'.format(city)]\n",
    "        df_final.loc[position, 'temp_range_{}'.format(city)] = abs(temp_max - temp_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing that there is a high correlation among the temperatures of the different cities, we will also try creating a weighted temperature features by taking into account each city's population [[7]](#ref40).\n",
    "\n",
    "- Madrid: 6,155,116\n",
    "- Barcelona: 5,179,243\n",
    "- Valencia: 1,645,342\n",
    "- Seville: 1,305,342\n",
    "- Bilbao: 987,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the weight of every city\n",
    "\n",
    "total_pop = 6155116 + 5179243 + 1645342 + 1305342 + 987000\n",
    "\n",
    "weight_Madrid = 6155116 / total_pop\n",
    "weight_Barcelona = 5179243 / total_pop\n",
    "weight_Valencia = 1645342 / total_pop\n",
    "weight_Seville = 1305342 / total_pop\n",
    "weight_Bilbao = 987000 / total_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_weights = {'Madrid': weight_Madrid, \n",
    "                  'Barcelona': weight_Barcelona,\n",
    "                  'Valencia': weight_Valencia,\n",
    "                  'Seville': weight_Seville,\n",
    "                  'Bilbao': weight_Bilbao}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 'temp_weighted' feature\n",
    "\n",
    "for i in range(len(df_final)):\n",
    "    position = df_final.index[i]\n",
    "    temp_weighted = 0\n",
    "    for city in cities:\n",
    "        temp = df_final.loc[position, 'temp_{}'.format(city)]\n",
    "        temp_weighted += temp * cities_weights.get('{}'.format(city))\n",
    "    df_final.loc[position, 'temp_weighted'] = temp_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also generate a new feature which aggregates both energy sources which are related to coal and are highly correlated (0.7688)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['generation coal all'] = df_final['generation fossil hard coal'] + df_final['generation fossil brown coal/lignite']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref17'>3.2. Feature selection</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_data(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "        \n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "        \n",
    "        if single_step:\n",
    "            labels.append(target[i + target_size])\n",
    "        else:\n",
    "            labels.append(target[i : i + target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_end_idx = 27048\n",
    "cv_end_idx = 31056\n",
    "test_end_idx = 35064"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_final[df_final.columns.drop('price actual')].values\n",
    "y = df_final['price actual'].values\n",
    "\n",
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('electricity_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_X.fit(X[:train_end_idx])\n",
    "scaler_y.fit(y[:train_end_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = scaler_X.transform(X)\n",
    "y_norm = scaler_y.transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_pca = pca.fit(X_norm[:train_end_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = len(pca.explained_variance_ratio_)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(np.arange(num_components), pca.explained_variance_ratio_)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Principal component')\n",
    "plt.ylabel('Explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.80)\n",
    "pca.fit(X_norm[:train_end_idx])\n",
    "X_pca = pca.transform(X_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_pca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset_norm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((\u001b[43mX_pca\u001b[49m, y_norm), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_final\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataset_norm)\n\u001b[0;32m      4\u001b[0m past_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_pca' is not defined"
     ]
    }
   ],
   "source": [
    "dataset_norm = np.concatenate((X_pca, y_norm), axis=1)\n",
    "np.save(\"dataset_final\", dataset_norm)\n",
    "\n",
    "past_history = 24\n",
    "future_target = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27024, 24, 17)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = multivariate_data(dataset_norm, dataset_norm[:, -1],\n",
    "                                     0, train_end_idx, past_history, \n",
    "                                     future_target, step=1, single_step=True)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = multivariate_data(dataset_norm, dataset_norm[:, -1],\n",
    "                                 train_end_idx, cv_end_idx, past_history, \n",
    "                                 future_target, step=1, single_step=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = multivariate_data(dataset_norm, dataset_norm[:, -1],\n",
    "                                   cv_end_idx, test_end_idx, past_history, \n",
    "                                   future_target, step=1, single_step=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "buffer_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('data/clean_and_merge_data.csv', index_col=0)\n",
    "\n",
    "data_splits = {\n",
    "    \"train\": 0.7,\n",
    "    \"val\": 0.15,\n",
    "    \"predict\": 0.15\n",
    "}\n",
    "\n",
    "elec_dm = ElectricityDataModule(\n",
    "    dataset_splits=data_splits,\n",
    "    batch_size=32,\n",
    "    window_size=24\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 24, 76)\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "elec_dm.setup(stage=\"fit\")\n",
    "dl = elec_dm.train_dataloader()\n",
    "\n",
    "x_torch, y_torch = np.zeros((32, 24, 76)), np.zeros((32, 1))\n",
    "for i, (input, output) in enumerate(dl):\n",
    "    if i == idx:\n",
    "        print(input.numpy())\n",
    "        x_torch = input.numpy()\n",
    "        y_torch = output.numpy()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 24, 17)\n"
     ]
    }
   ],
   "source": [
    "train = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train = train.cache().shuffle(buffer_size).batch(batch_size).prefetch(1)\n",
    "\n",
    "x_tf = list(train.as_numpy_iterator())[idx][0]\n",
    "y_tf = list(train.as_numpy_iterator())[idx][1]\n",
    "print(x_tf.shape)\n",
    "\n",
    "validation = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "validation = validation.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print((x_torch.shape == x_tf.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some common parameters\n",
    "\n",
    "input_shape = X_train.shape[-2:]\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "metric = [tf.keras.metrics.RootMeanSquaredError()]\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "              lambda epoch: 1e-4 * 10**(epoch / 10))\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reshape(-1, 1)\n",
    "y_test_inv = scaler_y.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='ref18'>4. Electricity Price Forecasting</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_rmse_and_loss(history):\n",
    "    \n",
    "    # Evaluate train and validation accuracies and losses\n",
    "    \n",
    "    train_rmse = history.history['root_mean_squared_error']\n",
    "    val_rmse = history.history['val_root_mean_squared_error']\n",
    "    \n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    # Visualize epochs vs. train and validation accuracies and losses\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_rmse, label='Training RMSE')\n",
    "    plt.plot(val_rmse, label='Validation RMSE')\n",
    "    plt.legend()\n",
    "    plt.title('Epochs vs. Training and Validation RMSE')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Epochs vs. Training and Validation Loss')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref19'>4.1. Naive Forecast</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref20'>4.2. XGBoost</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_xgb = X_train.reshape(-1, X_train.shape[1] * X_train.shape[2])\n",
    "X_val_xgb = X_val.reshape(-1, X_val.shape[1] * X_val.shape[2])\n",
    "X_test_xgb = X_test.reshape(-1, X_test.shape[1] * X_test.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'eta': 0.03, 'max_depth': 180, \n",
    "         'subsample': 1.0, 'colsample_bytree': 0.95, \n",
    "         'alpha': 0.1, 'lambda': 0.15, 'gamma': 0.1,\n",
    "         'objective': 'reg:linear', 'eval_metric': 'rmse', \n",
    "         'silent': 1, 'min_child_weight': 0.1, 'n_jobs': -1}\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train_xgb, y_train)\n",
    "dval = xgb.DMatrix(X_val_xgb, y_val)\n",
    "dtest = xgb.DMatrix(X_test_xgb, y_test)\n",
    "eval_list = [(dtrain, 'train'), (dval, 'eval')]\n",
    "\n",
    "xgb_model = xgb.train(param, dtrain, 180, eval_list, early_stopping_rounds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = xgb_model.predict(dtest)\n",
    "xgb_forecast = forecast.reshape(-1, 1)\n",
    "\n",
    "xgb_forecast_inv = scaler_y.inverse_transform(xgb_forecast)\n",
    "\n",
    "rmse_xgb = sqrt(mean_squared_error(y_test_inv, xgb_forecast_inv))\n",
    "print('RMSE of hour-ahead electricity price XGBoost forecast: {}'\n",
    "      .format(round(rmse_xgb, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref21'>4.3. LSTM</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "multivariate_lstm = tf.keras.models.Sequential([\n",
    "    LSTM(100, input_shape=input_shape, \n",
    "         return_sequences=True),\n",
    "    Flatten(),\n",
    "    Dense(200, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                   'multivariate_lstm.h5', monitor=('val_loss'), save_best_only=True)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=6e-3, amsgrad=True)\n",
    "\n",
    "multivariate_lstm.compile(loss=loss,\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = multivariate_lstm.fit(train, epochs=120,\n",
    "                                validation_data=validation,\n",
    "                                callbacks=[early_stopping, \n",
    "                                           model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_rmse_and_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_lstm = tf.keras.models.load_model('multivariate_lstm.h5')\n",
    "\n",
    "forecast = multivariate_lstm.predict(X_test)\n",
    "lstm_forecast = scaler_y.inverse_transform(forecast)\n",
    "\n",
    "rmse_lstm = sqrt(mean_squared_error(y_test_inv,\n",
    "                                    lstm_forecast))\n",
    "print('RMSE of hour-ahead electricity price LSTM forecast: {}'\n",
    "      .format(round(rmse_lstm, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref22'>4.4. Stacked LSTM</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "multivariate_stacked_lstm = tf.keras.models.Sequential([\n",
    "    LSTM(250, input_shape=input_shape, \n",
    "         return_sequences=True),\n",
    "    LSTM(150, return_sequences=True),\n",
    "    Flatten(),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                   'multivariate_stacked_lstm.h5', save_best_only=True)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=3e-3, amsgrad=True)\n",
    "\n",
    "multivariate_stacked_lstm.compile(loss=loss,\n",
    "                                  optimizer=optimizer,\n",
    "                                  metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = multivariate_stacked_lstm.fit(train, epochs=120,\n",
    "                                validation_data=validation,\n",
    "                                callbacks=[early_stopping, \n",
    "                                           model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_rmse_and_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_stacked_lstm = tf.keras.models.load_model('multivariate_stacked_lstm.h5')\n",
    "\n",
    "forecast = multivariate_stacked_lstm.predict(X_test)\n",
    "multivariate_stacked_lstm_forecast = scaler_y.inverse_transform(forecast)\n",
    "\n",
    "rmse_mult_stacked_lstm = sqrt(mean_squared_error(y_test_inv, \n",
    "                                                 multivariate_stacked_lstm_forecast))\n",
    "print('RMSE of hour-ahead electricity price multivariate Stacked LSTM forecast: {}'\n",
    "      .format(round(rmse_mult_stacked_lstm, 3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref23'>4.5. CNN</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "multivariate_cnn = tf.keras.models.Sequential([\n",
    "    Conv1D(filters=48, kernel_size=2,\n",
    "           strides=1, padding='causal',\n",
    "           activation='relu', \n",
    "           input_shape=input_shape),\n",
    "    Flatten(),\n",
    "    Dense(48, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                   'multivariate_cnn.h5', save_best_only=True)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=6e-3, amsgrad=True)\n",
    "\n",
    "multivariate_cnn.compile(loss=loss,\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = multivariate_cnn.fit(train, epochs=120,\n",
    "                               validation_data=validation,\n",
    "                               callbacks=[early_stopping, \n",
    "                                          model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_rmse_and_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_cnn = tf.keras.models.load_model('multivariate_cnn.h5')\n",
    "\n",
    "forecast = multivariate_cnn.predict(X_test)\n",
    "multivariate_cnn_forecast = scaler_y.inverse_transform(forecast)\n",
    "\n",
    "rmse_mult_cnn = sqrt(mean_squared_error(y_test_inv,\n",
    "                                        multivariate_cnn_forecast))\n",
    "print('RMSE of hour-ahead electricity price multivariate CNN forecast: {}'\n",
    "      .format(round(rmse_mult_cnn, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref24'>4.6. CNN-LSTM</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "multivariate_cnn_lstm = tf.keras.models.Sequential([\n",
    "    Conv1D(filters=100, kernel_size=2,\n",
    "           strides=1, padding='causal',\n",
    "           activation='relu', \n",
    "           input_shape=input_shape),\n",
    "    LSTM(100, return_sequences=True),\n",
    "    Flatten(),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                   'multivariate_cnn_lstm.h5', save_best_only=True)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=4e-3, amsgrad=True)\n",
    "\n",
    "multivariate_cnn_lstm.compile(loss=loss,\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = multivariate_cnn_lstm.fit(train, epochs=120,\n",
    "                                    validation_data=validation,\n",
    "                                    callbacks=[early_stopping, \n",
    "                                               model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_rmse_and_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_cnn_lstm = tf.keras.models.load_model('multivariate_cnn_lstm.h5')\n",
    "\n",
    "forecast = multivariate_cnn_lstm.predict(X_test)\n",
    "multivariate_cnn_lstm_forecast = scaler_y.inverse_transform(forecast)\n",
    "\n",
    "rmse_mult_cnn_lstm = sqrt(mean_squared_error(y_test_inv, \n",
    "                                             multivariate_cnn_lstm_forecast))\n",
    "print('RMSE of hour-ahead electricity price multivariate CNN-`LSTM forecast: {}'\n",
    "      .format(round(rmse_mult_cnn_lstm, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref25'>4.7. Time Distributed MLP</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "multivariate_mlp = tf.keras.models.Sequential([\n",
    "    TimeDistributed(Dense(200, activation='relu'),\n",
    "                    input_shape=input_shape),\n",
    "    TimeDistributed(Dense(150, activation='relu')),\n",
    "    TimeDistributed(Dense(100, activation='relu')),\n",
    "    TimeDistributed(Dense(50, activation='relu')),\n",
    "    Flatten(),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                   'multivariate_mlp.h5', save_best_only=True)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=2e-3, amsgrad=True)\n",
    "\n",
    "multivariate_mlp.compile(loss=loss,\n",
    "                          optimizer=optimizer,\n",
    "                          metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = multivariate_mlp.fit(train, epochs=120,\n",
    "                               validation_data=validation,\n",
    "                               callbacks=[early_stopping, \n",
    "                                          model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_rmse_and_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_mlp = tf.keras.models.load_model('multivariate_mlp.h5')\n",
    "\n",
    "forecast = multivariate_mlp.predict(X_test)\n",
    "multivariate_mlp_forecast = scaler_y.inverse_transform(forecast)\n",
    "\n",
    "rmse_mult_mlp = sqrt(mean_squared_error(y_test_inv,\n",
    "                                        multivariate_mlp_forecast))\n",
    "print('RMSE of hour-ahead electricity price multivariate MLP forecast: {}'\n",
    "      .format(round(rmse_mult_mlp, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='ref26'>4.8. Encoder-Decoder</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "encoder_decoder = tf.keras.models.Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=input_shape),\n",
    "    RepeatVector(past_history),\n",
    "    LSTM(50, activation='relu', return_sequences=True),\n",
    "    TimeDistributed(Dense(50, activation='relu')),\n",
    "    Flatten(),\n",
    "    Dense(25, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                   'encoder_decoder.h5', save_best_only=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-3, amsgrad=True)\n",
    "\n",
    "encoder_decoder.compile(loss=loss,\n",
    "                    optimizer=optimizer,\n",
    "                    metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = encoder_decoder.fit(train, epochs=50,\n",
    "                              validation_data=validation,\n",
    "                              callbacks=[early_stopping, \n",
    "                                         model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_rmse_and_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_decoder = tf.keras.models.load_model('encoder_decoder.h5')\n",
    "\n",
    "forecast = encoder_decoder.predict(X_test)\n",
    "encoder_decoder_forecast = scaler_y.inverse_transform(forecast)\n",
    "\n",
    "rmse_encoder_decoder = sqrt(mean_squared_error(y_test_inv, \n",
    "                                               encoder_decoder_forecast))\n",
    "print('RMSE of hour-ahead electricity price Encoder-Decoder forecast: {}'\n",
    "      .format(round(rmse_encoder_decoder, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='ref27'>5. Results</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='ref10'>6. References</a>\n",
    "- <a id='ref1'>[1]</a> Ortiz, M.; Ukar, O.; Azevedo, F. and Múgica, A. (2016). Price forecasting and validation in the Spanish electricity\n",
    "market using forecasts as input data, ​ International Journal of Electrical Power & Energy Systems​ , 77 : 123-127.\n",
    "- <a id='ref5'>[2]</a> https://en.wikipedia.org/wiki/Orders_of_magnitude_(pressure)\n",
    "- <a id='ref6'>[3]</a> https://en.wikipedia.org/wiki/Orders_of_magnitude_(speed)\n",
    "- <a id='ref7'>[4]</a> https://en.wikipedia.org/wiki/Fujita_scale\n",
    "- <a id='ref8'>[5]</a> https://en.wikipedia.org/wiki/List_of_European_tornadoes_and_tornado_outbreaks\n",
    "- <a id='ref9'>[6]</a> https://en.wikipedia.org/wiki/List_of_atmospheric_pressure_records_in_Europe#Iberia\n",
    "- <a id='ref40'>[7]</a> https://en.wikipedia.org/wiki/List_of_metropolitan_areas_in_Spain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ** Thank you for your time! If you like this notebook, **$\\color{magenta}{\\text{PLEASE UPVOTE!}}$** Do not hestitate to share your thoughts and do not forget to check again, as the notebook will be updated soon! **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
